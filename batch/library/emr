#!/usr/bin/env python

"""
Script for deploying and terminating EMR clusters, using boto3.

For use with EMR ami_version 2.x and 3.x, and EMR release labels 4.0 and up.

"""

import boto3
import os
import random
import re
import time
import traceback
from past.builtins import basestring

POLLING_INTERVAL_SECONDS = 30

DEFAULT_REGION = 'us-east-1'
DEFAULT_AMI_VERSION = 'latest'
DEFAULT_INSTANCE_GROUPS = {}

MASTER_ROLE = u'master'
CORE_ROLE = u'core'

# For a state flow diagram see - http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/ProcessingCycle.html
ALIVE_STATES = ['STARTING', 'BOOTSTRAPPING', 'RUNNING', 'WAITING', 'TERMINATING']
TERMINAL_STATES = ['TERMINATED', 'TERMINATED_WITH_ERRORS']

# Define values that get configured separately but get mapped to the Instance substructure.
SECURITY_GROUPS = {
    'emr_managed_master_security_group': 'EmrManagedMasterSecurityGroup',
    'emr_managed_slave_security_group': 'EmrManagedSlaveSecurityGroup',
    'additional_master_security_groups': 'AdditionalMasterSecurityGroups',
    'additional_slave_security_groups': 'AdditionalSlaveSecurityGroups',
}


class ElasticMapreduceCluster():

    def __init__(self, name=None, region=DEFAULT_REGION):
        self.name = name
        self._emr = boto3.client('emr', region)
        self.cluster_id = self._find_named_cluster(name)
        self.is_instance_fleet = False

    def _find_named_cluster(self, name):
        for cur_cluster in self._emr.list_clusters(ClusterStates=ALIVE_STATES).get('Clusters', []):
            if name == cur_cluster.get('Name'):
                return cur_cluster.get('Id')
        return None

    def provision_if_necessary(
        self,
        timeout=3000,
        **remaining_args
    ):
        changed = False
        params = self.get_run_jobflow_parameters(**remaining_args)
        if self.cluster_id is None:
            response = self._emr.run_job_flow(**params)
            self.cluster_id = response.get('JobFlowId')
            changed = True

        self.wait_for_cluster_to_launch(timeout)
        return changed

    def get_run_jobflow_parameters(self, **remaining_args):
        arguments = dict(remaining_args)
        parameters = self.get_boto_base_specs(arguments)

        parameters['Instances'] = self.get_boto_instance_specs(arguments)

        instance_fleets = arguments.pop('instance_fleets', {})
        instance_groups = arguments.pop('instance_groups', DEFAULT_INSTANCE_GROUPS)

        ebs_root_volume_size = arguments.pop('ebs_root_volume_size')

        if ebs_root_volume_size:
            parameters['EbsRootVolumeSize'] = int(ebs_root_volume_size)

        if not instance_fleets:
            self.is_instance_fleet = False
            parameters['Instances']['InstanceGroups'] = self.get_boto_instance_group_specs(instance_groups)
        else:
            self.is_instance_fleet = True
            parameters['Instances']['InstanceFleets'] = self.get_boto_instance_fleet_specs(instance_fleets)

        bootstrap_actions = arguments.pop('bootstrap_actions', None)
        if bootstrap_actions is not None:
            parameters['BootstrapActions'] = self.get_boto_bootstrap_action_specs(bootstrap_actions)

        steps = arguments.pop('steps', None)
        if steps is not None:
            parameters['Steps'] = self.get_boto_step_specs(steps)

        return parameters

    def get_boto_base_specs(self, arguments):
        ami_version = arguments.pop('ami_version', DEFAULT_AMI_VERSION)
        release_label = arguments.pop('release_label')
        custom_ami = arguments.pop('custom_ami')
        log_uri = arguments.pop('log_uri')
        job_flow_role = arguments.pop('job_flow_role')
        service_role = arguments.pop('service_role')
        visible_to_all_users = arguments.pop('visible_to_all_users', False)
        tags = arguments.pop('tags') or []
        tags.append({'Key': 'ansible:emr:name', 'Value': self.name})
        applications = arguments.pop('applications', None)
        configurations = arguments.pop('configurations', None)

        parameters = {
            'Name': self.name,
            'LogUri': log_uri,
            'VisibleToAllUsers': visible_to_all_users,
            'JobFlowRole': job_flow_role,
            'ServiceRole': service_role,
            'Instances': [],
            'Steps': [],
            'BootstrapActions': [],
            'Tags': tags,
        }

        # If a release label is provided, ignore the AMI version.
        if release_label:
            parameters['ReleaseLabel'] = release_label
            if applications is not None:
                parameters['Applications'] = self.get_boto_application_specs(applications)
            if configurations is not None:
                parameters['Configurations'] = self.get_boto_configuration_specs(configurations)
            ec2_attributes = arguments.pop('ec2_attributes')
            if ec2_attributes:
                parameters['Ec2Attributes'] = ec2_attributes
        else:
            parameters['AmiVersion'] = ami_version
            if applications is not None:
                parameters['NewSupportedProducts'] = self.get_boto_application_specs(applications)

        if custom_ami:
            parameters['CustomAmiId'] = custom_ami

        return parameters

    def get_boto_instance_specs(self, arguments):
        keypair_name = arguments.pop('keypair_name', os.getenv('AWS_KEYPAIR_NAME', None))
        if keypair_name is None:
            raise ValueError('Either keypair_name must be provided or AWS_KEYPAIR_NAME must be set in the environment.')

        instance_specs = {
            'Ec2KeyName': keypair_name,
            'KeepJobFlowAliveWhenNoSteps': True,
            'TerminationProtected': False,
        }

        # Specify either Availability zone or subnet, but not both.
        availability_zone = arguments.pop('availability_zone', None)
        vpc_subnet_id = arguments.pop('vpc_subnet_id', None)
        if vpc_subnet_id and availability_zone:
            raise ValueError('Must specify only one of availability_zone or vpc_subnet_id.')

        if vpc_subnet_id:
          if isinstance(vpc_subnet_id, list):
            # Amazon claims to allocate the best subnet if a list is passed in, but we are seeing all creations
            # going to the same subnet, which is causing startup failures when we run out of IPs. Trying to
            # randomize the list in the hopes that it will spread things out better.
            instance_specs['Ec2SubnetIds'] = vpc_subnet_id
            random.shuffle(instance_specs['Ec2SubnetIds'])
          elif isinstance(vpc_subnet_id, basestring):
            instance_specs['Ec2SubnetId'] = vpc_subnet_id
          else:
            raise ValueError(
                'vpc_subnet_id must be either a list or str. Found a {}.'.format(
                    type(vpc_subnet_id).__name__,
                )
            )
        elif availability_zone:
            instance_specs['Placement'] = {'AvailabilityZone': availability_zone}

        hadoop_version = arguments.pop('hadoop_version', None)
        if hadoop_version:
            instance_specs['HadoopVersion'] = hadoop_version

        for arg_name, key_name in SECURITY_GROUPS.items():
            arg_value = arguments.pop(arg_name, None)
            if arg_value is not None:
                instance_specs[key_name] = arg_value

        return instance_specs

    def has_ebs_configuration(self, args):
        return 'volume_size' in args or 'volume_type' in args

    def get_ebs_configuration_from_args(self, args):
        # In GB - beware of EBS minimum sizes for different volume types.
        volume_size = int(args.get('volume_size', 32))

        # st1, gp2, io1, or standard
        volume_type = args.get('volume_type', 'gp2')

        return {
            'EbsBlockDeviceConfigs': [
                {
                    'VolumeSpecification': {
                        'VolumeType': volume_type,
                        'SizeInGB': volume_size,
                      # 'Iops': number,  # IO operations per second
                    },
                  # 'VolumesPerInstance': 1,
                },
            ],
            'EbsOptimized': True,
        }

    def get_boto_instance_group_specs(self, instance_group_configs):
        instance_group_specs = []
        for role, args in instance_group_configs.items():
            num_instances = int(args['num_instances'])
            if num_instances > 0:
                instance_group = {
                    'Name': role,
                    'InstanceRole': role.upper(),
                    'InstanceType': args['type'],
                    'InstanceCount': num_instances,
                }
                if 'market' in args:
                    instance_group['Market'] = args['market']
                    if 'bidprice' in args:
                        # Make sure the float is converted to a string.
                        instance_group['BidPrice'] = str(args['bidprice'])

                # Specify the EBS configuration if given.
                if self.has_ebs_configuration(args):
                    instance_group['EbsConfiguration'] = self.get_ebs_configuration_from_args(args)

                instance_group_specs.append(instance_group)

        return instance_group_specs

    def get_boto_instance_fleet_specs(self, instance_fleet_configs):
        instance_fleet_specs = []
        for role, args in instance_fleet_configs.items():
            # These control whether or not we'll get on-demand instances in place of
            # spot instances when the spot market bids can't be fulfilled, and how long
            # EMR will wait, in minutes, before doing so.
            on_demand_fallback = args.get('on_demand_fallback', False)
            bid_timeout_after = args.get('bid_timeout_after', 15)
            global_bid_price_pct = args.get('bid_price_pct', 25)

            instance_fleet = {
                'Name': role,
                'InstanceFleetType': role.upper(),
            }

            spot_capacity = args.get('spot_capacity', 0)
            on_demand_capacity = args.get('on_demand_capacity', 0)

            # if both spot_capacity and on_demand_capacity for core instance fleet are zero, skip it
            # entirely from launch specifications.
            if role == CORE_ROLE:
                if spot_capacity == 0 and on_demand_capacity == 0:
                    continue

            # When specifying an instance fleet for the master role, you can only specify either spot
            # or on-demand, and not more than a value of one.  This logic will let you simply specify
            # either:
            #   master: { type: m3.xlarge }
            # to get an on-demand instance or:
            #   master: { type: m3.xlarge, use_spot: False }
            # to get a spot instance.
            use_spot = args.get('use_spot', True)

            launch_spec = {
                'SpotSpecification': {
                    'TimeoutAction': 'SWITCH_TO_ON_DEMAND' if on_demand_fallback else 'TERMINATE_CLUSTER',
                    'TimeoutDurationMinutes': bid_timeout_after,
                }
            }

            if role == MASTER_ROLE:
                if use_spot:
                    instance_fleet['TargetSpotCapacity'] = 1
                    instance_fleet['LaunchSpecifications'] = launch_spec
                else:
                    instance_fleet['TargetOnDemandCapacity'] = 1
            else:
                if use_spot:
                    instance_fleet['TargetOnDemandCapacity'] = on_demand_capacity
                    instance_fleet['TargetSpotCapacity'] = spot_capacity
                    instance_fleet['LaunchSpecifications'] = launch_spec
                else:
                    instance_fleet['TargetOnDemandCapacity'] = on_demand_capacity

            # Grab the instance type, or list of instance types, to configure ourselves for.
            instance_types = []
            configured_instance_types = args.get('instance_types', [])
            if not isinstance(configured_instance_types, list):
                configured_instance_types = [configured_instance_types]

            # Specify the EBS configuration if given.
            ebs_config = None
            if self.has_ebs_configuration(args):
                ebs_config = self.get_ebs_configuration_from_args(args)

            for configured_instance_type in configured_instance_types:
                local_bid_price_pct = configured_instance_type.get('bid_price_pct', global_bid_price_pct)
                instance_type = {
                    'InstanceType': configured_instance_type['type'],
                    'BidPriceAsPercentageOfOnDemandPrice': local_bid_price_pct,
                    'WeightedCapacity': configured_instance_type.get('weight', 1),
                }

                if ebs_config:
                    instance_type['EbsConfiguration'] = ebs_config

                instance_types.append(instance_type)

            instance_fleet['InstanceTypeConfigs'] = instance_types

            instance_fleet_specs.append(instance_fleet)

        return instance_fleet_specs

    def get_boto_bootstrap_action_specs(self, bootstrap_action_configs):
        bootstrap_action_specs = []
        for name, spec in bootstrap_action_configs.items():
            path = spec
            args = []
            if isinstance(spec, dict):
                path = spec.get('path')
                args = spec.get('args', [])

            action = {
                'Name': name,
                'ScriptBootstrapAction': {
                    'Path': path,
                    'Args': args,
                }
            }
            bootstrap_action_specs.append(action)

        return bootstrap_action_specs

    def get_boto_step_specs(self, step_configs):
        steps = []
        for step_config in step_configs:
            step_type = step_config.pop('type', 'jar')

            # Specify the path to the jar.
            if 'jar' in step_config:
                step_jar = step_config.pop('jar')
            elif step_type == 'streaming':
                step_jar = '/home/hadoop/contrib/streaming/hadoop-streaming.jar'
            else:
                step_jar = 's3://elasticmapreduce/libs/script-runner/script-runner.jar'

            step = {
                'Name': step_config.pop('name', step_type),
                'HadoopJarStep': {
                    'Jar': step_jar,
                }
            }

            # Add optional arguments.
            if 'main_class' in step_config:
                step['HadoopJarStep']['MainClass'] = step_config['main_class']
            if 'properties' in step_config:
                step['HadoopJarStep']['Properties'] = step_config['properties']
            if 'action_on_failure' in step_config:
                step['ActionOnFailure'] = step_config['action_on_failure']

            # Add support for specific step types.
            if step_type in ['hive_install']:
                step['HadoopJarStep']['Args'] = self.get_boto_install_hive_step_args(step_config)
            elif step_type in ['pig_install']:
                step['HadoopJarStep']['Args'] = self.get_boto_install_pig_step_args(step_config)
            elif 'step_args' in step_config:
                step['HadoopJarStep']['Args'] = step_config['step_args']

            steps.append(step)

        return steps

    def get_boto_install_hive_step_args(self, step_config):
        hive_versions = step_config.pop('hive_version', 'latest')
        hive_site = step_config.pop('hive_site', None)
        args = [
            's3://elasticmapreduce/libs/hive/hive-script',
            '--base-path',
            's3://elasticmapreduce/libs/hive/',
            '--install-hive',
            '--hive-versions',
            hive_versions,
        ]
        if hive_site is not None:
            args.append('--hive-site=%s' % hive_site)
        return args

    def get_boto_install_pig_step_args(self, step_config):
        return [
            's3://elasticmapreduce/libs/pig/pig-script',
            '--base-path',
            's3://elasticmapreduce/libs/pig/',
            '--install-pig',
            '--pig-versions',
            step_config.pop('pig_version', 'latest'),
        ]

    def get_boto_application_specs(self, application_configs):
        applications = []
        if application_configs is None:
            application_configs = []

        for app_config in application_configs:
            app = {
                'Name': app_config['name'],
                'Args': [],
            }
            if 'args' in app_config:
                app['Args'] = app_config['args']
            applications.append(app)

        return applications

    def get_boto_configuration_specs(self, configs):
        def get_configuration_spec(config):
            spec = {
                'Classification': config['classification']
            }
            if 'properties' in config:
                spec['Properties'] = config['properties']
            if 'configurations' in config:
                nested_specs = []
                for nested_config in config['configurations']:
                    nested_specs.append(get_configuration_spec(nested_config))
                spec['Configurations'] = nested_specs
            return spec

        configuration_specs = []
        for config in configs:
            spec = get_configuration_spec(config)
            configuration_specs.append(spec)

        return configuration_specs

    def wait_for_cluster_to_launch(self, timeout):
        wait_timeout = time.time() + timeout
        while wait_timeout > time.time() and not self.cluster_is_ready():

            if not self.cluster_is_alive():
                raise RuntimeError('Job flow failed to start.')

            time.sleep(POLLING_INTERVAL_SECONDS)

        if not self.cluster_is_ready():
            # If the cluster is only partially built, terminate any residual machines
            # before quitting
            self.terminate_if_necessary(timeout)
            raise RuntimeError('Timeout waiting for job flow to initialize.')

    def cluster_is_ready(self, cluster_id=None):
        cluster_id = cluster_id or self.cluster_id
        state = self.get_cluster_state(cluster_id)

        if state not in ['WAITING']:
            return False
        else:
            if self.is_instance_fleet:
                response = self._emr.list_instance_fleets(ClusterId=cluster_id)
                for instance_fleet in response['InstanceFleets']:
                    if instance_fleet['Status']['State'] != 'RUNNING':
                        return False
                return True
            else:
                response = self._emr.list_instance_groups(ClusterId=cluster_id)
                for instance_group in response['InstanceGroups']:
                    if instance_group['Status']['State'] != 'RUNNING':
                        return False
                return True

    def get_cluster_state(self, cluster_id=None):
        cluster_id = cluster_id or self.cluster_id
        cluster = self._emr.describe_cluster(ClusterId=cluster_id)
        return cluster['Cluster']['Status']['State']

    def cluster_is_alive(self, cluster_id=None):
        state = self.get_cluster_state(cluster_id)
        return state not in TERMINAL_STATES

    def terminate_if_necessary(self, timeout=3000):
        changed = False
        if self.cluster_id is not None:
            self._emr.terminate_job_flows(JobFlowIds=[self.cluster_id])
            changed = True

            self.wait_for_cluster_to_terminate(timeout)
        return changed

    def wait_for_cluster_to_terminate(self, timeout):
        wait_timeout = time.time() + timeout
        while wait_timeout > time.time() and self.cluster_is_alive():
            time.sleep(POLLING_INTERVAL_SECONDS)

        if self.cluster_is_alive():
            raise RuntimeError('Timeout waiting for job flow to terminate.')

    def get_metadata(self):
        cluster = self._emr.describe_cluster(ClusterId=self.cluster_id)
        public_dns_name = cluster['Cluster']['MasterPublicDnsName']
        result = self._emr.list_instances(ClusterId=self.cluster_id, InstanceGroupTypes=['MASTER'])
        # Assume that there is only one master instance.  Since there may be failures in creating
        # instances before there is a success, choose the last one.
        master_instance = result['Instances'][-1]
        private_ip_address = master_instance['PrivateIpAddress']
        # public_dns_name should equal master_instance['PublicDnsName'].
        return {
            'jobflow_id': self.cluster_id,
            'master_public_dns_name': public_dns_name,
            'master_private_ip': private_ip_address,
         }


def main():
    module = AnsibleModule(
        argument_spec = dict(
            state                = dict(default='present', type='str'),
            region               = dict(default='us-east-1', type='str'),
            # Top-level parameters:
            name                 = dict(required=True, type='str'),
            log_uri              = dict(default=None, type='str'),
            ami_version          = dict(type='str'),
            release_label        = dict(default=None, type='str'),
            custom_ami           = dict(default=None, type='str'),
            visible_to_all_users = dict(type='bool'),
            job_flow_role        = dict(default=None, type='str'),
            service_role         = dict(default='EMR_DefaultRole', type='str'),
            # Instance-level parameters:
            instance_fleets      = dict(default=None, type='dict'),
            instance_groups      = dict(type='dict'),
            keypair_name         = dict(type='str'),
            availability_zone    = dict(type='str'),
            hadoop_version       = dict(default=None, type='str'),
            vpc_subnet_id        = dict(default=None, type='raw'),
            emr_managed_master_security_group = dict(default=None, type='str'),
            emr_managed_slave_security_group = dict(default=None, type='str'),
            additional_master_security_groups = dict(default=None, type='list', elements='str'),
            additional_slave_security_groups = dict(default=None, type='list', elements='str'),
            # Additional parameters:
            ebs_root_volume_size = dict(type='str'),
            bootstrap_actions    = dict(type='dict'),
            steps                = dict(type='list', elements='dict'),
            tags                 = dict(type='list', elements='dict'),
            applications         = dict(type='list', elements='dict'),
            # Release-label parameters:
            configurations       = dict(type='list', elements='dict'),
            ec2_attributes       = dict(type='dict'),
        )
    )
    arguments = dict(module.params)
    name = arguments.pop('name')
    region = arguments.pop('region')
    desired_state = arguments.pop('state')

    try:
        cluster = ElasticMapreduceCluster(name, region)
        if desired_state == 'present':
            changed = cluster.provision_if_necessary(**arguments)
            metadata = cluster.get_metadata()
            metadata['changed'] = changed
            module.exit_json(**metadata)
        elif desired_state == 'absent':
            changed = cluster.terminate_if_necessary()
            module.exit_json(changed=changed)
        else:
            raise ValueError('Unknown state requested: {0}'.format(desired_state))
    except Exception:
        module.fail_json(msg=traceback.format_exc())


# import module snippets
from ansible.module_utils.basic import *
main()
